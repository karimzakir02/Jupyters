{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Your Own Latent (BYOL)\n",
    "## By: Karim Zakir\n",
    "BYOL is a representation learning method. The idea behind this method is that the same image with two different augmentations has the same \"essence\" and thus should have the same or similar embeddings. This method was originally published in [this paper](https://arxiv.org/abs/2006.07733v3) in January 2020. \n",
    "\n",
    "Contrastive learning is a deep learning technique in which we try to learn representations of different objects with the goal that embeddings of similar objects will also be similar. Most contrastive learning architectures/methods require \"negative pairs\", which make constrastive learning computationally expensive. Unlike those methods, BYOL does not require negative pairs, so it's a lot more efficient!\n",
    " \n",
    "In BYOL, we start off by taking an image and applying two different augmentations on it ($t$ and $t'$). These two images are fed to two different networks called online and target. Let's first focus on the online network. After applying augmentation $t$ on a given image, we feed it into the first part of the online network, which is an encoder, which could be any network that transforms an image to features. frf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_augmentation():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224)),  # interpolation should be BICUBIC, but outputs NAN for some reason\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "\n",
    "        transforms.RandomApply(\n",
    "            [transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)], p=0.8\n",
    "        ),\n",
    "\n",
    "        transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.2),\n",
    "\n",
    "        transforms.GaussianBlur((23, 23)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def target_augmentation():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224)),  # interpolation should be BICUBIC, but outputs NAN for some reason\n",
    "        \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "\n",
    "        transforms.RandomApply(\n",
    "            [transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)], p=0.8\n",
    "        ),\n",
    "\n",
    "        transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.2),\n",
    "\n",
    "        transforms.RandomApply([transforms.GaussianBlur((23, 23))], p=0.1),\n",
    "\n",
    "        transforms.RandomSolarize(threshold=0.5, p=0.2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "\n",
    "    def __init__(self, online_augmentation, target_augmentation, encoder_model):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.online_augmentation = online_augmentation\n",
    "        self.target_augmentation = target_augmentation\n",
    "    \n",
    "        self.encoder = encoder_model\n",
    "        self.encoder.fc = nn.Identity()\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 256)\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(256, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 256)\n",
    "        )\n",
    "\n",
    "        self.online_network = nn.Sequential(\n",
    "            self.encoder,\n",
    "            self.projector,\n",
    "            self.predictor\n",
    "        )\n",
    "\n",
    "        self.target_network = deepcopy(nn.Sequential(\n",
    "            self.encoder,\n",
    "            self.projector\n",
    "        ))\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters())\n",
    "        self.tau_base = 0.996\n",
    "        self.tau = self.tau_base\n",
    "\n",
    "    def fit(self, train_loader, val_loader, epochs=1000, verbose=True):\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            self.train(train_loader)\n",
    "\n",
    "            self.tau += 1 - (1 - self.tau_base) * (math.cos(math.pi*(epoch + 1) / epochs) + 1)/2\n",
    "\n",
    "            train_loss.append(self.validate(train_loader))\n",
    "            val_loss.append(self.validate(val_loader))\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            print(f\"Train Loss: {train_loss}\")\n",
    "            print(f\"Validation Loss: {val_loss}\")\n",
    "        \n",
    "        return train_loss, val_loss\n",
    "\n",
    "    def train(self, train_loader):\n",
    "\n",
    "        self.online_network.train()\n",
    "        self.target_network.train()\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            batch_loss = self.forward(batch_X)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update target's parameters\n",
    "            for target_param, online_param in zip(self.online_network.parameters(), self.target_network.parameters()):\n",
    "                target_param = self.tau * target_param + (1 - self.tau) * online_param\n",
    "            print(\"finished batch\")\n",
    "\n",
    "\n",
    "        self.online_network.eval()\n",
    "        self.target_network.eval()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        view = self.online_augmentation(batch)\n",
    "        view_prime = self.target_augmentation(batch)\n",
    "\n",
    "        online_output, online_output_prime = self.online_network(view), self.online_network(view_prime)\n",
    "        online_output, online_output_prime = F.normalize(online_output), F.normalize(online_output_prime)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_output, target_output_prime = self.target_network(view_prime), self.target_network(view)\n",
    "            target_output, target_output_prime = F.normalize(target_output), F.normalize(target_output_prime)\n",
    "\n",
    "        loss = self.loss_fn(online_output, target_output) + self.loss_fn(online_output_prime, target_output_prime)\n",
    "        return loss\n",
    "    \n",
    "    def validate(self, loader):\n",
    "        loss = 0\n",
    "        count = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            with torch.no_grad():\n",
    "                loss += self.forward(batch)\n",
    "            count += len(batch)\n",
    "        \n",
    "        return loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "byol = BYOL(online_augmentation(), target_augmentation(), torchvision.models.resnet50())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_labeled = torchvision.datasets.STL10(\".\", split=\"train\", download=True, transform=transforms.ToTensor())\n",
    "train_unlabeled = torchvision.datasets.STL10(\".\", split=\"unlabeled\", download=True, transform=transforms.ToTensor())\n",
    "test_labeled = torchvision.datasets.STL10(\".\", split=\"test\", download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ll = DataLoader(train_labeled, BATCH_SIZE, shuffle=True)\n",
    "train_ul = DataLoader(train_unlabeled, BATCH_SIZE, shuffle=True)\n",
    "test_labeled = DataLoader(test_labeled, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 102760448 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zakir\\Documents\\Coding Projects\\Jupyters\\byol_network\\byol_network.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m byol\u001b[39m.\u001b[39;49mfit(train_ul, train_ll, \u001b[39m1000\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\zakir\\Documents\\Coding Projects\\Jupyters\\byol_network\\byol_network.ipynb Cell 8\u001b[0m in \u001b[0;36mBYOL.fit\u001b[1;34m(self, train_loader, val_loader, epochs, verbose)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m val_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau_base) \u001b[39m*\u001b[39m (math\u001b[39m.\u001b[39mcos(math\u001b[39m.\u001b[39mpi\u001b[39m*\u001b[39m(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m epochs) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(train_loader))\n",
      "\u001b[1;32mc:\\Users\\zakir\\Documents\\Coding Projects\\Jupyters\\byol_network\\byol_network.ipynb Cell 8\u001b[0m in \u001b[0;36mBYOL.train\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_X, batch_y \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     batch_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch_X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32mc:\\Users\\zakir\\Documents\\Coding Projects\\Jupyters\\byol_network\\byol_network.ipynb Cell 8\u001b[0m in \u001b[0;36mBYOL.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m view \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monline_augmentation(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m view_prime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_augmentation(batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m online_output, online_output_prime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monline_network(view), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monline_network(view_prime)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m online_output, online_output_prime \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(online_output), F\u001b[39m.\u001b[39mnormalize(online_output_prime)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zakir/Documents/Coding%20Projects/Jupyters/byol_network/byol_network.ipynb#X25sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer3(x)\n\u001b[0;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    144\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[1;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    147\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\zakir\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 102760448 bytes."
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = byol.fit(train_ul, train_ll, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836bca15fb3131664fa986416905c3f6b7c72a580a461949970047754da3200b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
